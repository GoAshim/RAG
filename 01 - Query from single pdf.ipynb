{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d015281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "180a12a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the environment variable containing the OpenAI API Key so we don't have to hardcode that here\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2ef80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an OpenAI object with LLM\n",
    "# We are using 'gpt-4o-mini' because it's cost effective and capable enough to query on text\n",
    "# Refer https://platform.openai.com/docs/models/gpt-4o-mini for more info\n",
    "\n",
    "OpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d24c5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the SimpleDirectoryReader from LlamaIndex to read the pdf file and store into Document object of LlamaIndex\n",
    "# SimpleDirectoryReader can read one or more files and supports various file types such as .csv, .docx, .pdf, and more.\n",
    "# Refer to https://github.com/run-llama/llama_index/blob/main/docs/docs/module_guides/loading/simpledirectoryreader.md\n",
    "\n",
    "docs = SimpleDirectoryReader(\"pdf/\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17d43eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 11:24:06,423 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# The VectorStoreIndex in LlamaIndex helps to store, query, and manage vector embeddings efficiently in RAG workflows. \n",
    "# Refer https://github.com/run-llama/llama_index/blob/main/docs/docs/module_guides/indexing/vector_store_index.md for more.\n",
    "# Here we will create the in-memory vector store from the document we created above from the pdf file\n",
    "# Make sure to have credit available in OpenAI account, otherwise it will give error saying \"You exceeded your current quota\"\n",
    "\n",
    "vector = VectorStoreIndex.from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa93bee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The as_query_engine method prepares the index for querying in natural language, for quick and intuitive data retrieval.\n",
    "# Refer https://docs.llamaindex.ai/en/stable/module_guides/indexing/lpg_index_guide/ for more\n",
    "# Create a query engine from the vector store\n",
    "\n",
    "query_engine = vector.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a84f8fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 12:08:00,967 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-09-02 12:08:02,472 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain and LlamaIndex differ in their approach to loading documents. LangChain may have a distinct method or process compared to LlamaIndex when it comes to document loading. The two systems likely have unique features or functionalities that set them apart in terms of how they handle document loading tasks.\n"
     ]
    }
   ],
   "source": [
    "# Perform a query using the query engine created above\n",
    "\n",
    "response = query_engine.query(\"Give me a comparison between loading documents using langchain and llamaindex\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
