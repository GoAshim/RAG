{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d015281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "180a12a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the environment variable containing the OpenAI API Key so we don't have to hardcode that here\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f2ef80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an OpenAI object with LLM\n",
    "# We are using 'gpt-4o-mini' because it's cost effective and capable enough to query on text\n",
    "# Refer https://platform.openai.com/docs/models/gpt-4o-mini for more info\n",
    "\n",
    "model = OpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d24c5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# We will use the SimpleDirectoryReader from LlamaIndex to read the text file and store into Document object of LlamaIndex\n",
    "# SimpleDirectoryReader can read one or more files and supports various file types such as .csv, .docx, .pdf, and more.\n",
    "# Refer to https://github.com/run-llama/llama_index/blob/main/docs/docs/module_guides/loading/simpledirectoryreader.md\n",
    "\n",
    "docs = SimpleDirectoryReader(\"./data/txt/\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17d43eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The VectorStoreIndex in LlamaIndex helps to store, query, and manage vector embeddings efficiently in RAG workflows. \n",
    "# Refer https://github.com/run-llama/llama_index/blob/main/docs/docs/module_guides/indexing/vector_store_index.md for more.\n",
    "# Here we will create the in-memory vector store from the document we created above from the pdf file\n",
    "# Make sure to have credit available in OpenAI account, otherwise it will give error saying \"You exceeded your current quota\"\n",
    "\n",
    "vector = VectorStoreIndex.from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa93bee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The as_chat_engine helps making a conversation with data with multiple back-and-forth instead of a single question & answer.\n",
    "# Refer https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/ for more\n",
    "# For standalone question over data without keeping track of conversation history, we should use Query Engine instead\n",
    "# Create a chat engine from the vector store using the best mode\n",
    "# Refer to https://docs.llamaindex.ai/en/stable/examples/chat_engine/chat_engine_best/ for different chat modes\n",
    "\n",
    "chat_engine = vector.as_chat_engine(chat_mode=\"best\", verbose=True, llm=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a84f8fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condensed question: What is the first program paul graham wrote?\n",
      "Paul Graham doesn't specifically mention the first program he wrote in the documents provided. However, he does recount his early experiences with programming on the IBM 1401, where he struggled to figure out what to do with it and didn't have any data stored on punched cards to work with. He also mentions that he couldn't remember any programs he wrote because they likely didn't do much. His first significant programming experience came later when he got a TRS-80 microcomputer, where he wrote simple games and a program to predict how high his model rockets would fly. But the exact first program he wrote is not detailed in the text.\n"
     ]
    }
   ],
   "source": [
    "# Chat using the chat engine created above\n",
    "\n",
    "response = chat_engine.chat(\"What is the first program paul graham wrote?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
